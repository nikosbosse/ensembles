% This file was created with Citavi 6.10.0.0

@article{.,
 abstract = {Answer to Kearns hypothesis boosting where is proven that such a setting is possible.},
 title = {The strength of weak learnability},
 url = {https://www.cs.princeton.edu/~schapire/papers/strengthofweak.pdf},
 urldate = {24.04.2020},
 file = {strengthofweak:Attachments/strengthofweak.pdf:application/pdf;The strength of weak learnability:Attachments/The strength of weak learnability.pdf:application/pdf}
}


@misc{Bonab.09.09.2017,
 abstract = {The number of component classifiers chosen for an ensemble greatly impacts the prediction ability. In this paper, we use a geometric framework for a priori determining the ensemble size, which is applicable to most of existing batch and online ensemble classifiers. There are only a limited number of studies on the ensemble size examining Majority Voting (MV) and Weighted Majority Voting (WMV). Almost all of them are designed for batch-mode, hardly addressing online environments. Big data dimensions and resource limitations, in terms of time and memory, make determination of ensemble size crucial, especially for online environments. For the MV aggregation rule, our framework proves that the more strong components we add to the ensemble, the more accurate predictions we can achieve. For the WMV aggregation rule, our framework proves the existence of an ideal number of components, which is equal to the number of class labels, with the premise that components are completely independent of each other and strong enough. While giving the exact definition for a strong and independent classifier in the context of an ensemble is a challenging task, our proposed geometric framework provides a theoretical explanation of diversity and its impact on the accuracy of predictions. We conduct a series of experimental evaluations to show the practical value of our theorems and existing challenges.},
 author = {Bonab, Hamed and Can, Fazli},
 date = {09.09.2017},
 title = {Less Is More: A Comprehensive Framework for the Number of Components of  Ensemble Classifiers},
 url = {https://arxiv.org/pdf/1709.02925},
 file = {Bonab, Can 09.09.2017 - Less Is More:Attachments/Bonab, Can 09.09.2017 - Less Is More.pdf:application/pdf}
}


@inproceedings{Bonab.2016,
 author = {Bonab, Hamed R. and Can, Fazli},
 title = {A Theoretical Framework on the Ideal Number of Classifiers for Online Ensembles in Data Streams},
 pages = {2053--2056},
 publisher = {{Assocation for Computing Machinery}},
 isbn = {9781450340731},
 editor = {Mukhopadhyay, Snehasis and Zhai, ChengXiang and Bertino, Elisa and Crestani, Fabio and Mostafa, Javed and Tang, Jie and Si, Luo and Zhou, Xiaofang and Chang, Yi and Li, Yunyao and Sondhi, Parikshit},
 booktitle = {CIKM'16},
 year = {2016},
 address = {New York, New York},
 doi = {10.1145/2983323.2983907},
 file = {Bonab, Can 10242016 - A Theoretical Framework:Attachments/Bonab, Can 10242016 - A Theoretical Framework.pdf:application/pdf}
}


@article{Bracher.2021,
 abstract = {For practical reasons, many forecasts of case, hospitalization, and death counts in the context of the current Coronavirus Disease 2019 (COVID-19) pandemic are issued in the form of central predictive intervals at various levels. This is also the case for the forecasts collected in the COVID-19 Forecast Hub (https://covid19forecasthub.org/). Forecast evaluation metrics like the logarithmic score, which has been applied in several infectious disease forecasting challenges, are then not available as they require full predictive distributions. This article provides an overview of how established methods for the evaluation of quantile and interval forecasts can be applied to epidemic forecasts in this format. Specifically, we discuss the computation and interpretation of the weighted interval score, which is a proper score that approximates the continuous ranked probability score. It can be interpreted as a generalization of the absolute error to probabilistic forecasts and allows for a decomposition into a measure of sharpness and penalties for over- and underprediction.},
 author = {Bracher, Johannes and Ray, Evan L. and Gneiting, Tilmann and Reich, Nicholas G.},
 year = {2021},
 title = {Evaluating epidemic forecasts in an interval format},
 pages = {e1008618},
 volume = {17},
 number = {2},
 journal = {PLoS computational biology},
 doi = {10.1371/journal.pcbi.1008618},
 file = {Bracher, Ray et al. 2021 - Evaluating epidemic forecasts:Attachments/Bracher, Ray et al. 2021 - Evaluating epidemic forecasts.pdf:application/pdf}
}


@article{Brownlee.27.12.2018,
 author = {Brownlee, Jason},
 year = {27.12.2018},
 title = {How to Develop a Weighted Average Ensemble for Deep Learning Neural Networks},
 url = {https://machinelearningmastery.com/weighted-average-ensemble-for-deep-learning-neural-networks/},
 urldate = {08.12.2021},
 journal = {Machine Learning Mastery},
 file = {Brownlee 27.12.2018 - How to Develop a Weighted:Attachments/Brownlee 27.12.2018 - How to Develop a Weighted.pdf:application/pdf}
}


@article{Buhlmann.2014,
 abstract = {This article is part of a For-Discussion-Section of Methods of Information in Medicine about the papers {\textquotedbl}The Evolution of Boosting Algorithms - From Machine Learning to Statistical Modelling{\textquotedbl} and {\textquotedbl}Extending Statistical Boosting - An Overview of Recent Methodological Developments{\textquotedbl}, written by Andreas Mayr and co-authors. It is introduced by an editorial. This article contains the combined commentaries invited to independently comment on the Mayr et al. papers. In subsequent issues the discussion can continue through letters to the editor.},
 author = {B{\"u}hlmann, P. and Gertheiss, J. and Hieke, S. and Kneib, T. and Ma, S. and Schumacher, M. and Tutz, G. and Wang, C-Y and Wang, Z. and Ziegler, A.},
 year = {2014},
 title = {Discussion of {\textquotedbl}the evolution of boosting algorithms{\textquotedbl} and {\textquotedbl}extending statistical boosting{\textquotedbl}},
 pages = {436--445},
 volume = {53},
 number = {6},
 journal = {Methods of information in medicine},
 doi = {10.3414/13100122},
 file = {10.3414@13100122:Attachments/10.3414@13100122.pdf:application/pdf}
}


@book{Fahrmeir.2013,
 author = {Fahrmeir, Ludwig and Kneib, Thomas. and Lang, Stefan and Marx, Brian},
 year = {2013},
 title = {Regression: Models, methods and applications},
 address = {New York},
 publisher = {Springer},
 isbn = {9783642343322},
 file = {KNEIB Regression  Modelle, Methoden und Anwendungen:Attachments/KNEIB Regression  Modelle, Methoden und Anwendungen.pdf:application/pdf}
}


@inproceedings{Gashler.2008,
 author = {Gashler, Mike and Giraud-Carrier, Christophe and Martinez, Tony},
 title = {Decision Tree Ensemble: Small Heterogeneous Is Better Than Large Homogeneous},
 pages = {900--905},
 publisher = {{IEEE Computer Society}},
 isbn = {978-0-7695-3495-4},
 editor = {Wani, M. A.},
 booktitle = {ICMLA 2008},
 year = {2008},
 address = {Los Alamitos Calif.},
 doi = {10.1109/ICMLA.2008.154},
 file = {Gashler, Giraud-Carrier et al. 12 11 2008 - 12 13 2008 - Decision Tree Ensemble:Attachments/Gashler, Giraud-Carrier et al. 12 11 2008 - 12 13 2008 - Decision Tree Ensemble.pdf:application/pdf}
}


@book{Hastie.2009,
 author = {Hastie, Trevor and Tibshirani, Robert and Friedman, J. H.},
 year = {2009},
 title = {The elements of statistical learning: Data mining, inference, and prediction /   Trevor Hastie, Robert Tibshirani, Jerome Friedman},
 price = {{\pounds}55.99},
 address = {New York},
 edition = {2nd ed.},
 publisher = {Springer},
 isbn = {0387848576},
 series = {Springer series in statistics},
 file = {2. ESLII{\_}print12:Attachments/2. ESLII{\_}print12.pdf:application/pdf}
}


@book{James.2013,
 author = {James, Gareth and Witten, Daniela and Hastie, Trevor and Tibshirani, Robert},
 year = {2013},
 title = {An introduction to statistical learning: With applications in R},
 keywords = {Mathematical models;Mathematical statistics;Problems, exercises, etc;R (Computer program language);Statistics},
 address = {New York},
 volume = {103},
 publisher = {Springer},
 isbn = {1461471370},
 series = {Springer texts in statistics},
 file = {1. ISLR Seventh Printing:Attachments/1. ISLR Seventh Printing.pdf:application/pdf}
}


@proceedings{Mukhopadhyay.2016,
 year = {2016},
 title = {CIKM'16: Proceedings of the 2016 ACM International Conference on Information and Knowledge Management : October 24-28, 2016, Indianapolis, IN, USA},
 address = {New York, New York},
 publisher = {{Assocation for Computing Machinery}},
 isbn = {9781450340731},
 editor = {Mukhopadhyay, Snehasis and Zhai, ChengXiang and Bertino, Elisa and Crestani, Fabio and Mostafa, Javed and Tang, Jie and Si, Luo and Zhou, Xiaofang and Chang, Yi and Li, Yunyao and Sondhi, Parikshit},
 institution = {{Association for Computing Machinery, publisher}},
 doi = {10.1145/2983323}
}


@article{Pawlikowski.2020,
 author = {Pawlikowski, Maciej and Chorowska, Agata},
 year = {2020},
 title = {Weighted ensemble of statistical models},
 pages = {93--97},
 volume = {36},
 number = {1},
 issn = {01692070},
 journal = {International Journal of Forecasting},
 doi = {10.1016/j.ijforecast.2019.03.019},
 file = {Pawlikowski, Chorowska 2020 - Weighted ensemble of statistical models:Attachments/Pawlikowski, Chorowska 2020 - Weighted ensemble of statistical models.pdf:application/pdf}
}


@article{Sollich.1995,
 author = {Sollich, Peter and Krogh, Anders},
 year = {1995},
 title = {Learning with ensembles: How overfitting can be useful},
 url = {https://papers.nips.cc/paper/1044-learning-with-ensembles-how-overfitting-can-be-useful},
 volume = {8},
 journal = {Advances in Neural Information Processing Systems},
 file = {Sollich, Krogh 1995 - Learning with ensembles:Attachments/Sollich, Krogh 1995 - Learning with ensembles.pdf:application/pdf}
}


@proceedings{Wani.2008,
 year = {2008},
 title = {ICMLA 2008: Seventh International Conference on Machine Learning and Applications  proceedings   11-13 Dec. 2008, San Diego, California},
 address = {Los Alamitos Calif.},
 publisher = {{IEEE Computer Society}},
 isbn = {978-0-7695-3495-4},
 editor = {Wani, M. A.},
 institution = {{California State University, Bakersfield} and {IEEE Systems, Man, and Cybernetics Society} and {Association for Machine Learning and Applications}}
}


@book{Wickham.2015,
 author = {Wickham, Hadley},
 year = {2015},
 title = {Advanced R},
 address = {Boca Raton, FL},
 publisher = {{CRC Press}},
 isbn = {9781466586963},
 series = {The R series},
 file = {Advanced-r Wickham:Attachments/Advanced-r Wickham.pdf:application/pdf}
}


